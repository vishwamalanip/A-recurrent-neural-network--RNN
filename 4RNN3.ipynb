{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07389624",
   "metadata": {},
   "source": [
    "# TYPES OF RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a4111",
   "metadata": {},
   "source": [
    "1.Many to One\n",
    "\n",
    " Many-to-One is used when a single output is required from multiple input units or a sequence of them. It takes a sequence of inputs to display a fixed output. Sentiment Analysis is a common example of this type of Recurrent Neural Network.\n",
    "\n",
    "for ex- output 1/0 or yes/no rating predictions: 1star/2star/3star/4star/5star\n",
    "\n",
    "2.One-to-Many\n",
    "\n",
    "One-to-Many is a type of RNN that gives multiple outputs when given a single input. It takes a fixed input size and gives a sequence of data outputs. Its applications can be found in Music Generation and Image Captioning.\n",
    "\n",
    "for ex-one input multiple outputs \n",
    "\n",
    "image captioning => image as an input , captions as an output\n",
    "\n",
    "3.many to many\n",
    "\n",
    "Many-to-Many is used to generate a sequence of output data from a sequence of input units.\n",
    "\n",
    "                    1>same length many to many                  2>variable length many to many\n",
    "                    \n",
    "                    In this case, the number of both            In this case, inputs and outputs have different \n",
    "      the input and output units is the same.(ex=name entity)      numbers of units.(machine translator)\n",
    "                                                                    encoder(input).   decoder(output)\n",
    "                \n",
    "4.one to one\n",
    "\n",
    "The simplest type of RNN is One-to-One, which allows a single input and a single output. It has fixed input and output sizes and acts as a traditional neural network. The One-to-One application can be found in Image Classification.(Example : cat or dog from picture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188c329",
   "metadata": {},
   "source": [
    "# BackPropogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7f514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0562f87",
   "metadata": {},
   "source": [
    "# Problems with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fca4c3",
   "metadata": {},
   "source": [
    "two major problems\n",
    "\n",
    "         1>problem of long term dependancy                            \n",
    "        first word pachad na words per depend kare, to                 \n",
    "        kyarek bau mota sentences naa lidhe or timesteps \n",
    "        vdhare hoito first word has short term memory\n",
    "        problem(Long-term dependencies are the situations\n",
    "        where the output of an RNN depends on the input that occurred \n",
    "        many time steps ago. For instance, consider the sentence \n",
    "        \"The cat, which was very hungry, ate the mouse\".\n",
    "        To understand the meaning of this sentence, you need to \n",
    "        remember that the cat is the subject of the verb ate,\n",
    "        even though they are separated by a long clause. \n",
    "        This is a long-term dependency, and it can affect\n",
    "        the performance of an RNN that tries to generate or analyze such sentences.)\n",
    "        \n",
    "        [ >>vanishing gradient problem\n",
    "        \n",
    "        >>exploding gradient problem  ]\n",
    "        \n",
    "        \n",
    "        2>Unstable gradients\n",
    "        The vanishing and exploding gradient problems occur when \n",
    "        the gradients become either too small or too large during \n",
    "        backpropagation. This can happen in RNNs because they have \n",
    "        recurrent connections that allow them to store information \n",
    "        from previous time steps. These connections create long-term\n",
    "        dependencies, which means that the gradient of a parameter \n",
    "        depends on many previous inputs and outputs. As a result, the \n",
    "        gradient can either multiply or decay exponentially as it travels back through time.\n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90df15",
   "metadata": {},
   "source": [
    "# solutions\n",
    "\n",
    "            1>different activation->relu/leaky relu\n",
    "            2> better weight unit\n",
    "            3>skip runs\n",
    "            4>LSTM\n",
    "            5>gradient cripping\n",
    "            6>controlled learning rate\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
